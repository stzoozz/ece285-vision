{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stzoozz/ece285-vision/blob/main/assignment4_release.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw6n8sqlFIjZ",
        "tags": [
          "pdf-title"
        ]
      },
      "source": [
        "# Assignment 4: Self-Attention for Vision\n",
        "\n",
        "For this assignment, we're going to implement self-attention blocks in a convolutional neural network for CIFAR-10 Classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWmQwMDlFIjj"
      },
      "source": [
        "# Part I. Preparation\n",
        "\n",
        "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LhcP-S1VFIjj",
        "tags": [
          "pdf-ignore"
        ]
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJJBVo0YFIjk",
        "outputId": "38b7071d-fb98-4427-b9d8-d586f4ffe5cb",
        "tags": [
          "pdf-ignore"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "NUM_TRAIN = 49000\n",
        "\n",
        "# The torchvision.transforms package provides tools for preprocessing data\n",
        "# and for performing data augmentation; here we set up a transform to\n",
        "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
        "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
        "transform = T.Compose([\n",
        "                T.ToTensor(),\n",
        "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "            ])\n",
        "\n",
        "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
        "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
        "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
        "# training set into train and val sets by passing a Sampler object to the\n",
        "# DataLoader telling how it should sample from the underlying Dataset.\n",
        "cifar10_train = dset.CIFAR10('./data/datasets', train=True, download=True,\n",
        "                             transform=transform)\n",
        "loader_train = DataLoader(cifar10_train, batch_size=64,\n",
        "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "cifar10_val = dset.CIFAR10('./data/datasets', train=True, download=True,\n",
        "                           transform=transform)\n",
        "loader_val = DataLoader(cifar10_val, batch_size=64,\n",
        "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "cifar10_test = dset.CIFAR10('./data/datasets', train=False, download=True,\n",
        "                            transform=transform)\n",
        "loader_test = DataLoader(cifar10_test, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An3cfS0tFIjm",
        "tags": [
          "pdf-ignore"
        ]
      },
      "source": [
        "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
        "\n",
        "The global variables `dtype` and `device` will control the data types throughout this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOu2LLoEFIjm",
        "outputId": "1ff9c90e-0110-40ed-96a2-cdbcff57e853",
        "tags": [
          "pdf-ignore-input"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float32 # we will be using float throughout this tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss\n",
        "print_every = 100\n",
        "\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0Cp2ir0FIjn",
        "tags": [
          "pdf-ignore"
        ]
      },
      "source": [
        "## Flatten Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4QkOBJdFIjo",
        "outputId": "5f9dee08-1b26-415a-ce7d-14bad3de749a",
        "tags": [
          "pdf-ignore-input"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before flattening:  tensor([[[[ 0,  1],\n",
            "          [ 2,  3],\n",
            "          [ 4,  5]]],\n",
            "\n",
            "\n",
            "        [[[ 6,  7],\n",
            "          [ 8,  9],\n",
            "          [10, 11]]]])\n",
            "After flattening:  tensor([[ 0,  1,  2,  3,  4,  5],\n",
            "        [ 6,  7,  8,  9, 10, 11]])\n"
          ]
        }
      ],
      "source": [
        "def flatten(x):\n",
        "    N = x.shape[0] # read in N, C, H, W\n",
        "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
        "\n",
        "def test_flatten():\n",
        "    x = torch.arange(12).view(2, 1, 3, 2)\n",
        "    print('Before flattening: ', x)\n",
        "    print('After flattening: ', flatten(x))\n",
        "\n",
        "test_flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGHNkBu-FIjr"
      },
      "source": [
        "### Check Accuracy Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zeM908EmFIjv"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F  # useful stateless functions\n",
        "def check_accuracy(loader, model):\n",
        "    if loader.dataset.train:\n",
        "        print('Checking accuracy on validation set')\n",
        "    else:\n",
        "        print('Checking accuracy on test set')\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            scores = model(x)\n",
        "            _, preds = scores.max(1)\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_samples += preds.size(0)\n",
        "        acc = float(num_correct) / num_samples\n",
        "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "        return 100 * acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ma0ntxxFIjv"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0kP6SeaiFIjv"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, epochs=1):\n",
        "    \"\"\"\n",
        "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
        "\n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "\n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    acc_max = 0\n",
        "    for e in range(epochs):\n",
        "        for t, (x, y) in enumerate(loader_train):\n",
        "\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "            scores = model(x)\n",
        "            loss = F.cross_entropy(scores, y)\n",
        "\n",
        "            # Zero out all of the gradients for the variables which the optimizer\n",
        "            # will update.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # This is the backwards pass: compute the gradient of the loss with\n",
        "            # respect to each  parameter of the model.\n",
        "            loss.backward()\n",
        "\n",
        "            # Actually update the parameters of the model using the gradients\n",
        "            # computed by the backwards pass.\n",
        "            optimizer.step()\n",
        "\n",
        "            if t % print_every == 0:\n",
        "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
        "                acc = check_accuracy(loader_val, model)\n",
        "                if acc >= acc_max:\n",
        "                    acc_max = acc\n",
        "                print()\n",
        "    print(\"Maximum accuracy attained: \", acc_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KNOHZjhJzwge"
      },
      "outputs": [],
      "source": [
        "# We need to wrap `flatten` function in a module in order to stack it\n",
        "# in nn.Sequential\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return flatten(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2dXHVezzwge"
      },
      "source": [
        "## Vanilla CNN; No Attention\n",
        "We implement the vanilla architecture for you here. Do not modify the architecture. You will use the same architecture in the following parts. Do not modify the hyper-parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypRQ4-mNzwge",
        "outputId": "38fdb695-8bb6-42d9-c373-5007fdc0128c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Iteration 0, loss = 2.3277\n",
            "Checking accuracy on validation set\n",
            "Got 138 / 1000 correct (13.80)\n",
            "\n",
            "Epoch 0, Iteration 100, loss = 1.4879\n",
            "Checking accuracy on validation set\n",
            "Got 466 / 1000 correct (46.60)\n",
            "\n",
            "Epoch 0, Iteration 200, loss = 1.3589\n",
            "Checking accuracy on validation set\n",
            "Got 509 / 1000 correct (50.90)\n",
            "\n",
            "Epoch 0, Iteration 300, loss = 1.2044\n",
            "Checking accuracy on validation set\n",
            "Got 530 / 1000 correct (53.00)\n",
            "\n",
            "Epoch 0, Iteration 400, loss = 1.2370\n",
            "Checking accuracy on validation set\n",
            "Got 553 / 1000 correct (55.30)\n",
            "\n",
            "Epoch 0, Iteration 500, loss = 1.1319\n",
            "Checking accuracy on validation set\n",
            "Got 564 / 1000 correct (56.40)\n",
            "\n",
            "Epoch 0, Iteration 600, loss = 1.1998\n",
            "Checking accuracy on validation set\n",
            "Got 585 / 1000 correct (58.50)\n",
            "\n",
            "Epoch 0, Iteration 700, loss = 1.1249\n",
            "Checking accuracy on validation set\n",
            "Got 587 / 1000 correct (58.70)\n",
            "\n",
            "Maximum accuracy attained:  58.699999999999996\n"
          ]
        }
      ],
      "source": [
        "channel_1 = 64\n",
        "channel_2 = 32\n",
        "learning_rate = 1e-3\n",
        "num_classes = 10\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, channel_1, 3, padding=1, stride=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(channel_1, channel_2, 3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    Flatten(),\n",
        "    nn.Linear(channel_2*32*32, num_classes),\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "train(model, optimizer, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTw89bPoFIjy"
      },
      "source": [
        "## Test set -- run this only once\n",
        "\n",
        "Now we test our model on the test set . Think about how this compares to your validation set accuracy.\n",
        "You should be able to see atleast 55% accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpj3j6aEFIjy",
        "outputId": "15c55b73-9c98-4dc2-ac5d-29279a8959e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking accuracy on test set\n",
            "Got 6008 / 10000 correct (60.08)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60.08"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "vanillaModel = model\n",
        "check_accuracy(loader_test, vanillaModel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gH8-iqGUOO1"
      },
      "source": [
        "## Part II Self-Attention\n",
        "\n",
        "In the next section, you will implement an Attention layer which you will then use within a convnet architecture defined above for cifar 10 classification task.\n",
        "\n",
        "A self-attention layer is formulated as following:\n",
        "\n",
        "Input: $X$ of shape $(H\\times W, C)$\n",
        "\n",
        "Query, key, value linear transforms are $W_Q$, $W_K$, $W_V$, of shape $(C, C)$. We implement these linear transforms as 1x1 convolutional layers of the same dimensions.\n",
        "\n",
        "$XW_Q$, $XW_K$, $XW_V$, represent the output volumes when input X is passed through the transforms.\n",
        "\n",
        "\n",
        "Self-Attention is given by the formula: $Attention(X) = X + Softmax(\\frac{XW_Q(XW_K)^\\top}{\\sqrt{C}})XW_V$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PeJsuJRzwge"
      },
      "source": [
        "### Inline Question 1: Self-Attention is equivalent to which of the following: (5 points)\n",
        "1. K-means clustering <br />\n",
        "2. Non-local means <br />\n",
        "3. Residual Block <br />\n",
        "4. Gaussian Blurring <br />\n",
        "\n",
        "Your Answer: Non-local means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgqeSIWhzwge"
      },
      "source": [
        "### Here you implement the Attention module, and run it in the next section (40 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uvcTf4eUY-FX"
      },
      "outputs": [],
      "source": [
        "# Initialize the attention module as a nn.Module subclass\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Implement the Key, Query and Value linear transforms as 1x1 convolutional layers\n",
        "        # Hint: channel size remains constant throughout\n",
        "        self.conv_query = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.conv_key = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.conv_value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "\n",
        "        # TODO: Pass the input through conv_query, reshape the output volume to (N, C, H*W)\n",
        "        q = self.conv_query(x)\n",
        "        q = q.view(N, C, H * W).permute(0, 2, 1)\n",
        "        # TODO: Pass the input through conv_key, reshape the output volume to (N, C, H*W)\n",
        "        k = self.conv_key(x)\n",
        "        k = k.view(N, C, H * W)\n",
        "        # TODO: Pass the input through conv_value, reshape the output volume to (N, C, H*W)\n",
        "        v = self.conv_value(x)\n",
        "        v = v.view(N, C, H * W).permute(0, 2, 1)\n",
        "        # TODO: Implement the above formula for attention using q, k, v, C\n",
        "        # NOTE: The X in the formula is already added for you in the return line\n",
        "\n",
        "        attention_scores = torch.bmm(q, k)\n",
        "        attention_scores = attention_scores / (C ** 0.5)\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        attention_output = torch.bmm(attention_weights, v)\n",
        "        attention_output = attention_output.permute(0, 2, 1)\n",
        "\n",
        "        # Reshape the output to (N, C, H, W) before adding to the input volume\n",
        "        attention = attention_output.reshape(N, C, H, W)\n",
        "        return x + attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqi4PwAKzwge"
      },
      "source": [
        "## Single Attention Block: Early attention; After the first conv layer. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A3xh7aVFWzM",
        "outputId": "fc88ee03-691d-40f4-e186-af7164ae7993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Iteration 0, loss = 2.3059\n",
            "Checking accuracy on validation set\n",
            "Got 121 / 1000 correct (12.10)\n",
            "\n",
            "Epoch 0, Iteration 100, loss = 1.4568\n",
            "Checking accuracy on validation set\n",
            "Got 455 / 1000 correct (45.50)\n",
            "\n",
            "Epoch 0, Iteration 200, loss = 1.3780\n",
            "Checking accuracy on validation set\n",
            "Got 491 / 1000 correct (49.10)\n",
            "\n",
            "Epoch 0, Iteration 300, loss = 1.3015\n",
            "Checking accuracy on validation set\n",
            "Got 548 / 1000 correct (54.80)\n",
            "\n",
            "Epoch 0, Iteration 400, loss = 1.4491\n",
            "Checking accuracy on validation set\n",
            "Got 550 / 1000 correct (55.00)\n",
            "\n",
            "Epoch 0, Iteration 500, loss = 1.2979\n",
            "Checking accuracy on validation set\n",
            "Got 571 / 1000 correct (57.10)\n",
            "\n",
            "Epoch 0, Iteration 600, loss = 1.2295\n",
            "Checking accuracy on validation set\n",
            "Got 602 / 1000 correct (60.20)\n",
            "\n",
            "Epoch 0, Iteration 700, loss = 1.1320\n",
            "Checking accuracy on validation set\n",
            "Got 593 / 1000 correct (59.30)\n",
            "\n",
            "Epoch 1, Iteration 0, loss = 1.1150\n",
            "Checking accuracy on validation set\n",
            "Got 592 / 1000 correct (59.20)\n",
            "\n",
            "Epoch 1, Iteration 100, loss = 1.0204\n",
            "Checking accuracy on validation set\n",
            "Got 608 / 1000 correct (60.80)\n",
            "\n",
            "Epoch 1, Iteration 200, loss = 1.2923\n",
            "Checking accuracy on validation set\n",
            "Got 636 / 1000 correct (63.60)\n",
            "\n",
            "Epoch 1, Iteration 300, loss = 0.9119\n",
            "Checking accuracy on validation set\n",
            "Got 626 / 1000 correct (62.60)\n",
            "\n",
            "Epoch 1, Iteration 400, loss = 0.9870\n",
            "Checking accuracy on validation set\n",
            "Got 635 / 1000 correct (63.50)\n",
            "\n",
            "Epoch 1, Iteration 500, loss = 0.7920\n",
            "Checking accuracy on validation set\n",
            "Got 648 / 1000 correct (64.80)\n",
            "\n",
            "Epoch 1, Iteration 600, loss = 0.8779\n",
            "Checking accuracy on validation set\n",
            "Got 641 / 1000 correct (64.10)\n",
            "\n",
            "Epoch 1, Iteration 700, loss = 1.0064\n",
            "Checking accuracy on validation set\n",
            "Got 658 / 1000 correct (65.80)\n",
            "\n",
            "Epoch 2, Iteration 0, loss = 0.7126\n",
            "Checking accuracy on validation set\n",
            "Got 650 / 1000 correct (65.00)\n",
            "\n",
            "Epoch 2, Iteration 100, loss = 0.9821\n",
            "Checking accuracy on validation set\n",
            "Got 641 / 1000 correct (64.10)\n",
            "\n",
            "Epoch 2, Iteration 200, loss = 0.9851\n",
            "Checking accuracy on validation set\n",
            "Got 653 / 1000 correct (65.30)\n",
            "\n",
            "Epoch 2, Iteration 300, loss = 0.9210\n",
            "Checking accuracy on validation set\n",
            "Got 660 / 1000 correct (66.00)\n",
            "\n",
            "Epoch 2, Iteration 400, loss = 0.8097\n",
            "Checking accuracy on validation set\n",
            "Got 663 / 1000 correct (66.30)\n",
            "\n",
            "Epoch 2, Iteration 500, loss = 0.8259\n",
            "Checking accuracy on validation set\n",
            "Got 681 / 1000 correct (68.10)\n",
            "\n",
            "Epoch 2, Iteration 600, loss = 1.1100\n",
            "Checking accuracy on validation set\n",
            "Got 651 / 1000 correct (65.10)\n",
            "\n",
            "Epoch 2, Iteration 700, loss = 0.6912\n",
            "Checking accuracy on validation set\n",
            "Got 671 / 1000 correct (67.10)\n",
            "\n",
            "Epoch 3, Iteration 0, loss = 0.6500\n",
            "Checking accuracy on validation set\n",
            "Got 677 / 1000 correct (67.70)\n",
            "\n",
            "Epoch 3, Iteration 100, loss = 0.6714\n",
            "Checking accuracy on validation set\n",
            "Got 660 / 1000 correct (66.00)\n",
            "\n",
            "Epoch 3, Iteration 200, loss = 0.7288\n",
            "Checking accuracy on validation set\n",
            "Got 675 / 1000 correct (67.50)\n",
            "\n",
            "Epoch 3, Iteration 300, loss = 0.6458\n",
            "Checking accuracy on validation set\n",
            "Got 675 / 1000 correct (67.50)\n",
            "\n",
            "Epoch 3, Iteration 400, loss = 0.6958\n",
            "Checking accuracy on validation set\n",
            "Got 678 / 1000 correct (67.80)\n",
            "\n",
            "Epoch 3, Iteration 500, loss = 0.8329\n",
            "Checking accuracy on validation set\n",
            "Got 679 / 1000 correct (67.90)\n",
            "\n",
            "Epoch 3, Iteration 600, loss = 1.1105\n",
            "Checking accuracy on validation set\n",
            "Got 687 / 1000 correct (68.70)\n",
            "\n",
            "Epoch 3, Iteration 700, loss = 0.6354\n",
            "Checking accuracy on validation set\n",
            "Got 683 / 1000 correct (68.30)\n",
            "\n",
            "Epoch 4, Iteration 0, loss = 0.5417\n",
            "Checking accuracy on validation set\n",
            "Got 688 / 1000 correct (68.80)\n",
            "\n",
            "Epoch 4, Iteration 100, loss = 0.6146\n",
            "Checking accuracy on validation set\n",
            "Got 690 / 1000 correct (69.00)\n",
            "\n",
            "Epoch 4, Iteration 200, loss = 0.5167\n",
            "Checking accuracy on validation set\n",
            "Got 698 / 1000 correct (69.80)\n",
            "\n",
            "Epoch 4, Iteration 300, loss = 0.5766\n",
            "Checking accuracy on validation set\n",
            "Got 682 / 1000 correct (68.20)\n",
            "\n",
            "Epoch 4, Iteration 400, loss = 0.5583\n",
            "Checking accuracy on validation set\n",
            "Got 685 / 1000 correct (68.50)\n",
            "\n",
            "Epoch 4, Iteration 500, loss = 0.6110\n",
            "Checking accuracy on validation set\n",
            "Got 681 / 1000 correct (68.10)\n",
            "\n",
            "Epoch 4, Iteration 600, loss = 0.5908\n",
            "Checking accuracy on validation set\n",
            "Got 679 / 1000 correct (67.90)\n",
            "\n",
            "Epoch 4, Iteration 700, loss = 0.4918\n",
            "Checking accuracy on validation set\n",
            "Got 687 / 1000 correct (68.70)\n",
            "\n",
            "Epoch 5, Iteration 0, loss = 0.3930\n",
            "Checking accuracy on validation set\n",
            "Got 690 / 1000 correct (69.00)\n",
            "\n",
            "Epoch 5, Iteration 100, loss = 0.5917\n",
            "Checking accuracy on validation set\n",
            "Got 704 / 1000 correct (70.40)\n",
            "\n",
            "Epoch 5, Iteration 200, loss = 0.5790\n",
            "Checking accuracy on validation set\n",
            "Got 684 / 1000 correct (68.40)\n",
            "\n",
            "Epoch 5, Iteration 300, loss = 0.4951\n",
            "Checking accuracy on validation set\n",
            "Got 688 / 1000 correct (68.80)\n",
            "\n",
            "Epoch 5, Iteration 400, loss = 0.5798\n",
            "Checking accuracy on validation set\n",
            "Got 681 / 1000 correct (68.10)\n",
            "\n",
            "Epoch 5, Iteration 500, loss = 0.7047\n",
            "Checking accuracy on validation set\n",
            "Got 671 / 1000 correct (67.10)\n",
            "\n",
            "Epoch 5, Iteration 600, loss = 0.4096\n",
            "Checking accuracy on validation set\n",
            "Got 682 / 1000 correct (68.20)\n",
            "\n",
            "Epoch 5, Iteration 700, loss = 0.5835\n",
            "Checking accuracy on validation set\n",
            "Got 693 / 1000 correct (69.30)\n",
            "\n",
            "Epoch 6, Iteration 0, loss = 0.4389\n",
            "Checking accuracy on validation set\n",
            "Got 679 / 1000 correct (67.90)\n",
            "\n",
            "Epoch 6, Iteration 100, loss = 0.5224\n",
            "Checking accuracy on validation set\n",
            "Got 675 / 1000 correct (67.50)\n",
            "\n",
            "Epoch 6, Iteration 200, loss = 0.3668\n",
            "Checking accuracy on validation set\n",
            "Got 688 / 1000 correct (68.80)\n",
            "\n",
            "Epoch 6, Iteration 300, loss = 0.6723\n",
            "Checking accuracy on validation set\n",
            "Got 704 / 1000 correct (70.40)\n",
            "\n",
            "Epoch 6, Iteration 400, loss = 0.3295\n",
            "Checking accuracy on validation set\n",
            "Got 690 / 1000 correct (69.00)\n",
            "\n",
            "Epoch 6, Iteration 500, loss = 0.4494\n",
            "Checking accuracy on validation set\n",
            "Got 684 / 1000 correct (68.40)\n",
            "\n",
            "Epoch 6, Iteration 600, loss = 0.4090\n",
            "Checking accuracy on validation set\n",
            "Got 689 / 1000 correct (68.90)\n",
            "\n",
            "Epoch 6, Iteration 700, loss = 0.4251\n",
            "Checking accuracy on validation set\n",
            "Got 678 / 1000 correct (67.80)\n",
            "\n",
            "Epoch 7, Iteration 0, loss = 0.3725\n",
            "Checking accuracy on validation set\n",
            "Got 680 / 1000 correct (68.00)\n",
            "\n",
            "Epoch 7, Iteration 100, loss = 0.2616\n",
            "Checking accuracy on validation set\n",
            "Got 691 / 1000 correct (69.10)\n",
            "\n",
            "Epoch 7, Iteration 200, loss = 0.4875\n",
            "Checking accuracy on validation set\n",
            "Got 697 / 1000 correct (69.70)\n",
            "\n",
            "Epoch 7, Iteration 300, loss = 0.2646\n",
            "Checking accuracy on validation set\n",
            "Got 676 / 1000 correct (67.60)\n",
            "\n",
            "Epoch 7, Iteration 400, loss = 0.3410\n",
            "Checking accuracy on validation set\n",
            "Got 687 / 1000 correct (68.70)\n",
            "\n",
            "Epoch 7, Iteration 500, loss = 0.4335\n",
            "Checking accuracy on validation set\n",
            "Got 679 / 1000 correct (67.90)\n",
            "\n",
            "Epoch 7, Iteration 600, loss = 0.4509\n",
            "Checking accuracy on validation set\n",
            "Got 693 / 1000 correct (69.30)\n",
            "\n",
            "Epoch 7, Iteration 700, loss = 0.5855\n",
            "Checking accuracy on validation set\n",
            "Got 684 / 1000 correct (68.40)\n",
            "\n",
            "Epoch 8, Iteration 0, loss = 0.2718\n",
            "Checking accuracy on validation set\n",
            "Got 679 / 1000 correct (67.90)\n",
            "\n",
            "Epoch 8, Iteration 100, loss = 0.3158\n",
            "Checking accuracy on validation set\n",
            "Got 677 / 1000 correct (67.70)\n",
            "\n",
            "Epoch 8, Iteration 200, loss = 0.2213\n",
            "Checking accuracy on validation set\n",
            "Got 676 / 1000 correct (67.60)\n",
            "\n",
            "Epoch 8, Iteration 300, loss = 0.2821\n",
            "Checking accuracy on validation set\n",
            "Got 679 / 1000 correct (67.90)\n",
            "\n",
            "Epoch 8, Iteration 400, loss = 0.3736\n",
            "Checking accuracy on validation set\n",
            "Got 682 / 1000 correct (68.20)\n",
            "\n",
            "Epoch 8, Iteration 500, loss = 0.3468\n",
            "Checking accuracy on validation set\n",
            "Got 683 / 1000 correct (68.30)\n",
            "\n",
            "Epoch 8, Iteration 600, loss = 0.4117\n",
            "Checking accuracy on validation set\n",
            "Got 667 / 1000 correct (66.70)\n",
            "\n",
            "Epoch 8, Iteration 700, loss = 0.3504\n",
            "Checking accuracy on validation set\n",
            "Got 675 / 1000 correct (67.50)\n",
            "\n",
            "Epoch 9, Iteration 0, loss = 0.2277\n",
            "Checking accuracy on validation set\n",
            "Got 688 / 1000 correct (68.80)\n",
            "\n",
            "Epoch 9, Iteration 100, loss = 0.1455\n",
            "Checking accuracy on validation set\n",
            "Got 689 / 1000 correct (68.90)\n",
            "\n",
            "Epoch 9, Iteration 200, loss = 0.2085\n",
            "Checking accuracy on validation set\n",
            "Got 676 / 1000 correct (67.60)\n",
            "\n",
            "Epoch 9, Iteration 300, loss = 0.2192\n",
            "Checking accuracy on validation set\n",
            "Got 669 / 1000 correct (66.90)\n",
            "\n",
            "Epoch 9, Iteration 400, loss = 0.3059\n",
            "Checking accuracy on validation set\n",
            "Got 664 / 1000 correct (66.40)\n",
            "\n",
            "Epoch 9, Iteration 500, loss = 0.2592\n",
            "Checking accuracy on validation set\n",
            "Got 677 / 1000 correct (67.70)\n",
            "\n",
            "Epoch 9, Iteration 600, loss = 0.3467\n",
            "Checking accuracy on validation set\n",
            "Got 673 / 1000 correct (67.30)\n",
            "\n",
            "Epoch 9, Iteration 700, loss = 0.3108\n",
            "Checking accuracy on validation set\n",
            "Got 679 / 1000 correct (67.90)\n",
            "\n",
            "Maximum accuracy attained:  70.39999999999999\n"
          ]
        }
      ],
      "source": [
        "channel_1 = 64\n",
        "channel_2 = 32\n",
        "learning_rate = 3e-4\n",
        "\n",
        "# TODO: Use the above Attention module after the first Convolutional layer.\n",
        "# Essentially the architecture should be [Conv->Relu->Attention->Relu->Conv->Relu->Linear]\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, channel_1, 3, padding=1, stride=1),\n",
        "    nn.ReLU(),\n",
        "    Attention(in_channels=channel_1), # attention after first conv\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(channel_1, channel_2, 3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    Flatten(),\n",
        "    nn.Linear(channel_2 * 32 * 32, 10)\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, optimizer, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYro5wGDzwge"
      },
      "source": [
        "## Test set -- run this only once\n",
        "\n",
        "Now we test our model on the test set . Think about how this compares to your validation set accuracy.\n",
        "You should see improvement of about 2-3% over the vanilla convnet model. * Use this part to tune your Attention module and then move on to the next parts. *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef5cbe0-13e6-469f-96c4-2fa75e630427",
        "id": "12UxlkjZzwgf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking accuracy on test set\n",
            "Got 6532 / 10000 correct (65.32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.32"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "earlyAttention = model\n",
        "check_accuracy(loader_test, earlyAttention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ6MxOtSzwgf"
      },
      "source": [
        "## Single Attention Block: Late attention; After the second conv layer. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0BH0Sb7zwgf"
      },
      "outputs": [],
      "source": [
        "channel_1 = 64\n",
        "channel_2 = 32\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# TODO: Use the above Attention module after the Second Convolutional layer.\n",
        "# Essentially the architecture should be [Conv->Relu->Conv->Relu->Attention->Relu->Linear]\n",
        "\n",
        "model = None\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, optimizer, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50JYi2Wqzwgf"
      },
      "source": [
        "## Test set -- run this only once\n",
        "\n",
        "Now we test our model on the test set . Think about how this compares to your validation set accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhk2LMsxzwgf"
      },
      "outputs": [],
      "source": [
        "lateAttention = model\n",
        "check_accuracy(loader_test, lateAttention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qI1bXXgzwgf"
      },
      "source": [
        "### Inline Question 2: Provide one example each of usage of self-attention and attention in computer vision. Explain the difference between the two. (5 points)\n",
        "\n",
        "\n",
        "Your Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pc9ei32zwgf"
      },
      "source": [
        "## Double Attention Blocks: After conv layers 1 and 2 (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlysvwUAzwgf"
      },
      "outputs": [],
      "source": [
        "channel_1 = 64\n",
        "channel_2 = 32\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# TODO: Use the above Attention module after the Second Convolutional layer.\n",
        "# Essentially the architecture should be [Conv->Relu->Attention->Relu->Conv->Relu->Attention->Relu->Linear]\n",
        "\n",
        "model = None\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, optimizer, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2u9Zu1Szwgf"
      },
      "source": [
        "## Test set -- run this only once\n",
        "\n",
        "Now we test our model on the test set . Think about how this compares to your validation set accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFWaW1JMzwgf"
      },
      "outputs": [],
      "source": [
        "vanillaModel = model\n",
        "check_accuracy(loader_test, vanillaModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__Vmi3yHzwgf"
      },
      "source": [
        "## Resnet with Attention\n",
        "\n",
        "Now we will experiment with applying attention within the Resnet10 architecture that we implemented in Homework 2. Please note that for a deeper model such as Resnet we do not expect significant improvements in performance with Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxqqZjlkzwgf"
      },
      "source": [
        "## Vanilla Resnet, No Attention\n",
        "\n",
        "The architecture for Resnet is given below, please train it and evaluate it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0tp-30Dzwgf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, img_channels=3, num_classes=100, batchnorm=False):\n",
        "        super(ResNet, self).__init__() #layers = [1, 1, 1, 1]\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(img_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.batchnorm = batchnorm\n",
        "        self.layer1 = self.make_layer(block, layers[0], out_channels=64, stride=1, batchnorm=batchnorm)\n",
        "        self.layer2 = self.make_layer(block, layers[1], out_channels=128, stride=1, batchnorm=batchnorm)\n",
        "        self.layer3 = self.make_layer(block, layers[2], out_channels=256, stride=1, batchnorm=batchnorm)\n",
        "        self.layer4 = self.make_layer(block, layers[3], out_channels=512, stride=2, batchnorm=batchnorm)\n",
        "\n",
        "        self.averagepool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.averagepool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def make_layer(self, block, num_blocks, out_channels, stride, batchnorm=False):\n",
        "        downsampler = None\n",
        "        layers = []\n",
        "        if stride != 1 or self.in_channels != out_channels:\n",
        "            downsampler = nn.Sequential(nn.Conv2d(self.in_channels, out_channels, kernel_size = 1, stride = stride), nn.BatchNorm2d(out_channels))\n",
        "\n",
        "        layers.append(block(self.in_channels, out_channels, downsampler, stride, batchnorm=batchnorm))\n",
        "\n",
        "        self.in_channels = out_channels\n",
        "\n",
        "        for i in range(num_blocks - 1):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "class block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, downsampler = None, stride = 1, batchnorm=False):\n",
        "\n",
        "        super(block, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 2)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = stride)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsampler = downsampler\n",
        "        self.relu = nn.ReLU()\n",
        "        self.batchnorm = batchnorm\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        residual = x\n",
        "        x = self.conv1(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        if self.downsampler:\n",
        "            residual = self.downsampler(residual)\n",
        "\n",
        "        return self.relu(residual + x)\n",
        "\n",
        "\n",
        "\n",
        "def ResNet10(num_classes = 100, batchnorm= False):\n",
        "\n",
        "    return ResNet(block, [1, 1, 1, 1], num_classes=num_classes, batchnorm=batchnorm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH9hPeZyzwgf"
      },
      "source": [
        "## Test set -- run this only once\n",
        "\n",
        "Now we test our model on the test set . Think about how this compares to your validation set accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAwT0aV6zwgf"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "model = ResNet10()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, optimizer, epochs=10)\n",
        "\n",
        "vanillaResnet = model\n",
        "check_accuracy(loader_test, vanillaResnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyvcnmsrzwgj"
      },
      "source": [
        "## Resnet with Attention (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cNGeK2Wzwgj"
      },
      "outputs": [],
      "source": [
        "## Resnet with Attention\n",
        "\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# TODO: Use the above Attention module after the 2nd resnet block i.e. after self.layer2.\n",
        "\n",
        "model = None\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, optimizer, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4t_AZPxzwgj"
      },
      "source": [
        "## Test set -- run this only once\n",
        "\n",
        "Now we test our model on the test set . Think about how this compares to your validation set accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lvjf4p3Qzwgj"
      },
      "outputs": [],
      "source": [
        "AttentionResnet = model\n",
        "check_accuracy(loader_test, AttentionResnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW-l2wQezwgj"
      },
      "source": [
        "## Inline Question 3: Rank the above models based on their performance on test dataset (15 points)\n",
        "( You are encouraged to run each of the experiments (training) at\n",
        "least 3 times to get an average estimate )\n",
        "\n",
        "Report the test accuracies alongside the model names. For example, 1. Vanilla CNN (57.45%, 57.99%).. etc\n",
        "\n",
        "1. <br />\n",
        "2. <br />\n",
        "3. <br />\n",
        "4. <br />\n",
        "5. <br />\n",
        "6. <br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIsEVxhezwgj"
      },
      "source": [
        "### Bonus Question (Ungraded): Can you give a possible explanation that supports the rankings?\n",
        "Your Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGltSn1jzwgj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "assignment3_solutions.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}